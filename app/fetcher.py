import requests
import os
from datetime import datetime, timedelta
import logging
from typing import Dict, Any, List, Optional, Callable
from bs4 import BeautifulSoup

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# âœ… API í‚¤ë¥¼ ê° ì„œë¹„ìŠ¤ì— ë§ê²Œ ë³„ë„ë¡œ ì„¤ì •
# í™˜ê²½ ë³€ìˆ˜ì— ê° API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”. (ì˜ˆ: UNION_TREND_API_KEY, UNION_OTHBC_API_KEY ë“±)
API_ENDPOINTS = {
    "ë¶í•œ ë™í–¥": {
        "key": os.environ.get("UNION_API_KEY"),
        "url": "http://apis.data.go.kr/1250000/trend/getTrend",
        "parser": lambda item: {"title": item.get("sj", ""), "content": item.get("cn", "")},
        "params": {"cl": "ARGUMENT_DAIL"}
    },
    "ê¹€ì •ì€ ê³µê°œ í™œë™": {
        "key": os.environ.get("UNION_API_KEY"),
        "url": "http://apis.data.go.kr/1250000/othbcact/getOthbcact",
        "parser": lambda item: {
            "title": item.get("nes_cn", "")[:100],  # ì œëª©ì€ ë³´ë„ë‚´ìš©ì˜ ì•ë¶€ë¶„ 100ìë¡œ ì„¤ì •
            "content": (
                f"ìˆ˜í–‰ì: {item.get('execman', 'ì •ë³´ ì—†ìŒ')}\n"
                f"ë³´ë„ì¼ì: {item.get('nes_ymd', 'ì •ë³´ ì—†ìŒ')}"
            )
        },
        "params": {}
    },
    "í†µì¼ë¶€ ë³´ë„ìë£Œ": {
        "key":os.environ.get("UNION_API_KEY"),
        "url": "http://apis.data.go.kr/1250000/nesdta/getNesdta",
        "parser": lambda item: {
            "title": item.get("sj", "").strip(), 
            "content": item.get("cn", "").strip() or item.get("sj", "").strip()
        },
        "params": {}
    }
}


def fetch_data_from_api(api_name: str, api_config: Dict[str, Any], start_date: str, end_date: str, max_items: int) -> Optional[str]:
    """
    ë‹¨ì¼ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì œë„¤ë¦­ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    service_key = api_config["key"]
    base_url = api_config["url"]
    parser = api_config.get("parser")
    extra_params = api_config.get("params", {})

    if not service_key:
        logger.error(f"âŒ '{api_name}' API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    # ëª¨ë“  APIì— ê³µí†µì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    params = {
        "serviceKey": service_key,
        "pageNo": 1,
        "numOfRows": max_items,
        "bgng_ymd": start_date,
        "end_ymd": end_date,
        "dataType": "JSON"
    }
    
    # APIë³„ ê³ ìœ  íŒŒë¼ë¯¸í„° ì¶”ê°€
    params.update(extra_params)
    
    try:
        logger.info(f"ğŸ”— '{api_name}' API ìš”ì²­ ì¤‘...")
        # âœ… verify=True ë˜ëŠ” ì œê±°í•˜ì—¬ SSL ê²€ì¦ í™œì„±í™”
        response = requests.get(base_url, params=params) 
        response.raise_for_status()
        logger.info(f"âœ… '{api_name}' API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        
        data = response.json()
        items = data.get("items", [])
        
        if not items:
            logger.warning(f"âš ï¸ '{api_name}'ì—ì„œ ìˆ˜ì‹ ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        logger.info(f"ğŸ“¦ '{api_name}' ìˆ˜ì§‘ëœ í•­ëª© ìˆ˜: {len(items)}")

        combined_text = ""
        for item in items:
            # âœ… ê° APIì˜ ë°˜í™˜ í˜•ì‹ì— ë§ëŠ” íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            parsed_item = parser(item)
            title = parsed_item.get("title", "")
            content = parsed_item.get("content", "")
            combined_text += f"[{title}]\n{content}\n\n"
        
        return combined_text
    
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ '{api_name}' API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ '{api_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

def scrape_articles_from_unikorea(target_date: str) -> List[Dict[str, str]]:
    """
    í†µì¼ë¶€ ë¶í•œì •ë³´í¬í„¸ì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ê¸°ì‚¬ë“¤ì„ ìŠ¤í¬ë©í•©ë‹ˆë‹¤.
    """
    base_url = "https://nkinfo.unikorea.go.kr/nkp/trend/"
    list_url = f"{base_url}list.do"
    
    scraped_articles = []
    
    try:
        logger.info(f"ğŸ”— í†µì¼ë¶€ ë¶í•œì •ë³´í¬í„¸ì—ì„œ {target_date} ê¸°ì‚¬ ìŠ¤í¬ë© ì‹œì‘...")
        response = requests.get(list_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # <table>ì—ì„œ ë“±ë¡ì¼ì´ target_dateì¸ í–‰ì„ ì°¾ìŠµë‹ˆë‹¤.
        rows = soup.select('table tbody tr')
        
        for row in rows:
            date_td = row.select_one('td:nth-child(3)')
            if date_td and date_td.text.strip() == target_date:
                trend_mng_no_element = row.find('a', class_='trendViewBtn')
                if trend_mng_no_element:
                    trend_mng_no = trend_mng_no_element.get('trendmngno')
                    
                    if trend_mng_no:
                        article_url = f"{base_url}view.do?menuId=&trendMngNo={trend_mng_no}"
                        
                        logger.info(f"ğŸ”— ê¸°ì‚¬ ë³¸ë¬¸ ìŠ¤í¬ë© ì¤‘: {article_url}")
                        article_response = requests.get(article_url)
                        article_response.raise_for_status()
                        
                        article_soup = BeautifulSoup(article_response.text, 'html.parser')
                        
                        title_element = article_soup.find('h4', id='trendTtl')
                        content_element = article_soup.find('div', id='index')
                        
                        title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"
                        content = content_element.get_text(separator='\n', strip=True) if content_element else "ë‚´ìš© ì—†ìŒ"
                        
                        scraped_articles.append({"title": title, "content": content})
                        logger.info(f"âœ… ê¸°ì‚¬ ìŠ¤í¬ë© ì™„ë£Œ: '{title}'")
        
        if not scraped_articles:
            logger.warning(f"âš ï¸ {target_date}ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ ì›¹ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

    return scraped_articles

def fetch_all_north_korea_trends(start_date=None, end_date=None, max_items=10) -> str:
    """
    ì •ì˜ëœ 3ê°œì˜ APIì™€ ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ìµœê·¼ 3ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì™€ í•©ì¹©ë‹ˆë‹¤.
    """
    today = datetime.today()
    # ìµœê·¼ 3ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì‹œì‘ì¼ì„ ì˜¤ëŠ˜ - 2ì¼ë¡œ ì„¤ì •
    start_date_dt = today - timedelta(days=2)
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = today.strftime("%Y%m%d")

    logger.info(f"ğŸ“¡ API ë° ìŠ¤í¬ë˜í•‘ ë™í–¥ ìˆ˜ì§‘ ì‹œì‘: {start_date} ~ {end_date}")
    
    all_combined_text = ""
    
    # 1. API ë°ì´í„° ìˆ˜ì§‘
    for api_name, api_config in API_ENDPOINTS.items():
        api_text = fetch_data_from_api(api_name, api_config, start_date, end_date, max_items)
        if api_text:
            all_combined_text += f"\n\n--- '{api_name}' ë°ì´í„° ì‹œì‘ ---\n\n"
            all_combined_text += api_text
            all_combined_text += f"\n--- '{api_name}' ë°ì´í„° ë ---\n"

    # 2. ìŠ¤í¬ë˜í•‘ ë°ì´í„° ìˆ˜ì§‘
    scraped_data_found = False
    scraped_text = ""
    for i in range(3):
        target_date_dt = today - timedelta(days=i)
        target_date_str = target_date_dt.strftime("%Y.%m.%d.")
        
        scraped_articles = scrape_articles_from_unikorea(target_date_str)
        
        if scraped_articles:
            scraped_data_found = True
            scraped_text += f"\n\n--- '{target_date_str}' ìŠ¤í¬ë© ë°ì´í„° ì‹œì‘ ---\n\n"
            for article in scraped_articles:
                scraped_text += f"[{article['title']}]\n{article['content']}\n\n"
            scraped_text += f"\n--- '{target_date_str}' ìŠ¤í¬ë© ë°ì´í„° ë ---\n"
    
    if scraped_data_found:
        all_combined_text += scraped_text
        
    if not all_combined_text.strip():
        logger.warning("âš ï¸ API ë° ìŠ¤í¬ë˜í•‘ ëª¨ë‘ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return "í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ ë¶í•œ ë™í–¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    logger.info("ğŸ“ ëª¨ë“  API ë° ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³‘í•© ì™„ë£Œ")
    return all_combined_text.strip()